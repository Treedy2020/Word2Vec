model_name: CBOW  # SkipGram or CBOW

dataset: WikiText2   # WikiText2 or WikiText103
data_dir: .data/  # root dir of the training data
train_batch_size: 96
val_batch_size: 96
shuffle: True

optimizer: Adam
learning_rate: 0.025
epochs: 5
train_steps: 10
val_steps: 10

checkpoint_frequency: 100
model_dir: weights//cbow_WikiText2  # save path for vocab.pt and model.pt
